---
title: "Practicle Machine Learning Week 2 Class Notes"
author: "Yigit Ozan Berk"
date: "9/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The caret package


## The caret R package

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/caret.png height=400>

[http://caret.r-forge.r-project.org/](http://caret.r-forge.r-project.org/)


---

## Caret functionality

* Some preprocessing (cleaning)
  * preProcess()
* Data splitting
  * createDataPartition()
  * createResample()
  * createTimeSlices()
* Training/testing functions
  * train()
  * predict()
* Model comparison
  * confusionMatrix()

---

## Machine learning algorithms in R

* Linear discriminant analysis(MASS package)
* Regression (stats package) 
* Naive Bayes
* Support vector machines
* Classification and regression trees
* Random forests
* Boosting
* etc. 

gbm (gbm package)
mda (mda package)
rpart (rpart package)
Weka (RWeka package)
LogitBoost (caTools package)

---

## Why caret? 

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/predicttable.png height=250>

[add link](??)


--- 

## SPAM Example: Data splitting

```{r loadPackage}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
#75 % to train and 25% to test
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```


--- 

## SPAM Example: Fit a model

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
modelFit
```


--- 

## SPAM Example: Final model

```{r finalModel, dependson="training",cache=TRUE}
modelFit <- train(type ~.,data=training, method="glm")
modelFit$finalModel
```


--- 

## SPAM Example: Prediction

```{r predictions, dependson="training",cache=TRUE}
predictions <- predict(modelFit,newdata=testing)
predictions
```

--- 

## SPAM Example: Confusion Matrix

```{r confusion, dependson="predictions",cache=TRUE}
confusionMatrix(predictions,testing$type)
```

---

## Further information

* Caret tutorials:
  * [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
  * [http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)
* A paper introducing the caret package
  * [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)
  


# Data Slicing



## SPAM Example: Data splitting

```{r loadPackage}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
# 75 % for training set, 25% for test set
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```

---

## SPAM Example: K-fold

```{r kfold,dependson="loadPackage"}
# for cross validation
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=TRUE)
#list = T means it will return each set of imbecies corresponding to a particular fold as a list. 
# returnTrain = T to return the training set
sapply(folds,length)
folds[[1]][1:10] #doesn't change the arrangement of the data
```

---

## SPAM Example: Return test

```{r kfoldtest,dependson="loadPackage"}
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=FALSE)
# to return the test set
sapply(folds,length)
folds[[1]][1:10] #for looking at the id of the sample
```

---

## SPAM Example: Resampling

```{r resample,dependson="loadPackage"}
set.seed(32323)
folds <- createResample(y=spam$type,times=10,
                             list=TRUE)
#for resampling or bootstrapping

sapply(folds,length)
folds[[1]][1:10]
#since we're doing resampling with replacement, you can get the same value multiple times
```

---

## SPAM Example: Time Slices

```{r time,dependson="loadPackage"}
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,
                          horizon=10)
# initialWindow = 20 == window of 20 samples in them
# horizon = 10 == i'm going to predict the next 10 samples after taking the initial window of 20 values
names(folds)
folds$train[[1]]
folds$test[[1]]
```


---

## Further information

* Caret tutorials:
  * [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
  for time slicing
  * [http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)
* A paper introducing the caret package
  * [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)




# Training Options


some of the training control options


## SPAM Example

```{r loadPackage,cache=TRUE}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
modelFit <- train(type ~.,data=training, method="glm")
```

---

## Train options

```{r ,dependson="loadPackage"}
args(train.default)
```
method
preProcess
weights
metric (Accuracy for binary, RMSE for continuous)
trControl = trainControl()


---

## Metric options

__Continous outcomes__:
  * _RMSE_ = Root mean squared error
  * _RSquared_ = $R^2$ from regression models

__Categorical outcomes__:
  * _Accuracy_ = Fraction correct
  * _Kappa_ = A measure of [concordance](http://en.wikipedia.org/wiki/Cohen%27s_kappa)
  
  

--- 

## trainControl

```{r , dependson="loadPackage",cache=TRUE}
args(trainControl)
```
method = "boot" == bootstrap or cross validation
number == number of times to do bootstrapping or cross valiadation
repeats == number of times to repeat cross validation
p = size of the training set 
other parameters for time course data initialWindow for number of time points that will be in the training data
horizon == number of time points that you'll be predicting
returning predictions themselves == savePredictions = T
pre proccessing options = preProcOptions
prediction bounds
seeds
parallelizing computations across multiple cores(for large number of samples.)



--- 

## trainControl resampling
for trainControl function
* _method_
  * _boot_ = bootstrapping
  * _boot632_ = bootstrapping with adjustment
  * _cv_ = cross validation
  * _repeatedcv_ = repeated cross validation
  * _LOOCV_ = leave one out cross validation
* _number_
  * For boot/cross validation
  * Number of subsamples to take
* _repeats_
  * Number of times to repeate subsampling
  * If big this can _slow things down_


---

## Setting the seed

* It is often useful to set an overall seed
* You can also set a seed for each resample
* Seeding each resample is useful for parallel fits
* _seed_ must be a list with
  * Length equal to number of resamples
  * Length of each element equal to number of models fit



--- 



## seed example

```{r seedExample, dependson="loadPackage",cache=TRUE}
set.seed(1235); seeds <- vector(26,mode="list")
for(i in 1:26){seeds[[i]] <- floor(runif(1,0,1e5))}
trControl <- trainControl(seeds=seeds)
```



--- 


## seed example

```{r , dependson="seedExample",cache=TRUE}
modelFit2 <- train(type ~.,data=training, method="glm")
modelFit2
```


--- 

## seed example

```{r , dependson="seedExample",cache=TRUE}
modelFit3 <- train(type ~.,data=training, method="glm")
modelFit3
```
  
  

do caret tutorial and model training and tuning tutorial







# Plotting Predictors




## Example: predicting wages

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/wages.jpg height=350>

Image Credit [http://www.cahs-media.org/the-high-cost-of-low-wages](http://www.cahs-media.org/the-high-cost-of-low-wages)

Data from: [ISLR package](http://cran.r-project.org/web/packages/ISLR) from the book: [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)  



---

## Example: Wage data

```{r loadData,cache=TRUE}
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)
```



---

## Get training/test sets

```{r trainingTest,dependson="loadData",cache=TRUE}
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```


---

## Feature plot (*caret* package)

```{r ,dependson="trainingTest",fig.height=4,fig.width=4}
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
#y is our outcome, featurePlot plots  outcome against each predictor
```


---

## Qplot (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,data=training)
#what is this strange relationship
```


---

## Qplot with color (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,colour=jobclass,data=training)
#check if job class makes a difference
```
most of the up chunk individuals are information based jobs

---

## Add regression smoothers (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qq <- qplot(age,wage,colour=education,data=training)
qq +  geom_smooth(method='lm',formula=y~x)
```


---

## cut2, making factors (*Hmisc* package)


```{r cut2,dependson="trainingTest",fig.height=4,fig.width=6,cache=TRUE}
install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
#let's see if different quantiles have different characteristic
```

---

## Boxplots with cut2


```{r ,dependson="cut2plot",fig.height=4,fig.width=6,cache=TRUE}
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot"))
p1
#let's see wage groups versus age
```

---

### Boxplots with points overlayed


```{r ,dependson="cut2plot",fig.height=4,fig.width=9}
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot","jitter"))
#boxplot and jitter together to see both boxplots and the points 
#to get a better sense of data points in the boxes 
library(gridExtra)
grid.arrange(p1,p2,ncol=2)
```


---

### Tables

```{r ,dependson="cut2",fig.height=4,fig.width=9}
t1 <- table(cutWage,training$jobclass)
t1
prop.table(t1,1)#proportions in each group
#check the data groups
```
more industrial and less information in lower wage jobs
more information and less industrial in higher wage jobs



---

### Density plots

```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(wage,colour=education,data=training,geom="density")
# like histogram
```


---

## Notes and further reading

* Make your plots only in the training set !!!
  * Don't use the test set for exploration!
* Things you should be looking for
  * Imbalance in outcomes/predictors 
  * Outliers (to find out if you're missing any variables)
  * Groups of points not explained by a predictor
  * Skewed variables (you may want to transform if you're using regression models)
* [ggplot2 tutorial](http://rstudio-pubs-static.s3.amazonaws.com/2176_75884214fc524dc0bc2a140573da38bb.html)
* [caret visualizations](http://caret.r-forge.r-project.org/visualizations.html)



# Preprocessing


You plotted variables and found out a very strange predictor or a strange distribution, and you need to deal with it.

## Why preprocess?

```{r loadPackage,cache=TRUE,fig.height=3.5,fig.width=3.5}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
```
almost all are very small but some are much larger.


---

## Why preprocess?

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
mean(training$capitalAve)
sd(training$capitalAve)
```

the data is very skewed and highly variable!

---

## Standardizing

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(trainCapAveS) # mean will be 0
sd(trainCapAveS) # sd will be 1
```
this will reduce a lot of the variability

---

## Standardizing - test set !!!!!!

when we apply a prediction algorithm to the test set, we can only use parameters that we estimated in the training set. 
when we apply the same standardization to the test set, we need to apply the same mean from the training set, and the same standart deviation from the training set!
```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(testCapAveS) #not zero because the mean used was of the training set.
sd(testCapAveS) # not 1 because the mean used was of the training set.
```


---

## Standardizing - _preProcess_ function

```{r preprocess,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
preObj <- preProcess(training[,-58],method=c("center","scale"))#58 is the actual outcome we cared about.
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```


---

## Standardizing - _preProcess_ function

```{r ,dependson="preprocess",cache=TRUE,fig.height=3.5,fig.width=3.5}
testCapAveS <- predict(preObj,testing[,-58])$capitalAve #then use the object created by the training set to preprocess the test set. (like using the mean of training set to center test set.)
mean(testCapAveS)
sd(testCapAveS)
```

---

## Standardizing - _preProcess_ argument

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
#directly passing the preProcess option to train function.
modelFit <- train(type ~.,data=training,
                  preProcess=c("center","scale"),method="glm")
modelFit
```


---

## Standardizing - Box-Cox transforms

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
preObj <- preProcess(training[,-58],method=c("BoxCox"))
#box cox transforms
#set of transformations that take continuous data, and try to make them look like normal data. by estimating a specific set of parameters using maximum likelihood.

trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
```
this is a continuous transform, if you have >0 data it cannot take care of repeated values ( values around 0 usually..).

---

## Standardizing - Imputing data

```{r knn,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
install.packages("RANN")
library(RANN)
set.seed(13343)
# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA
# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
```


---

## Standardizing - Imputing data

```{r ,dependson="knn",cache=TRUE,fig.height=3.5,fig.width=7}
quantile(capAve - capAveTruth)
#difference in values are mostly close to zero
quantile((capAve - capAveTruth)[selectNA])
#only for the ones that were missing
quantile((capAve - capAveTruth)[!selectNA])
```

---

## Notes and further reading

* Training and test must be processed in the same way !!!!
* Test transformations will likely be imperfect
  * Especially if the test/training sets collected at different times
* Careful when transforming factor variables!!!
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)




# Covariate Creation





## Two levels of covariate creation

**Level 1: From raw data to covariate**

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/covCreation1.png height=200>

**Level 2: Transforming tidy covariates** 

```{r spamData,fig.height=4,fig.width=4}
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2
```


---

## Level 1, Raw data -> covariates

* Depends heavily on application
* The balancing act is summarization vs. information loss
* Examples:
  * Text files: frequency of words, frequency of phrases ([Google ngrams](https://books.google.com/ngrams)), frequency of capital letters.
  * Images: Edges, corners, blobs, ridges ([computer vision feature detection](http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)))
  * Webpages: Number and type of images, position of elements, colors, videos ([A/B Testing](http://en.wikipedia.org/wiki/A/B_testing))
  * People: Height, weight, hair color, sex, country of origin. 
* The more knowledge of the system you have the better the job you will do. 
* When in doubt, err on the side of more features
* Can be automated, but use caution!


---

## Level 2, Tidy covariates -> new covariates

* More necessary for some methods (regression, svms) than for others (classification trees).
* Should be done _only on the training set_
* The best approach is through exploratory analysis (plotting/tables)
* New covariates should be added to data frames



---

## Load example data


```{r loadData,cache=TRUE}
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```


---

## Common covariates to add, dummy variables

__Basic idea - convert factor variables to [indicator variables](http://bit.ly/19ZhWB6)__

```{r dummyVar,dependson="loadData"}
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training)
head(predict(dummies,newdata=training))
```



---

## Removing zero covariates

```{r ,dependson="dummyVar"}
nsv <- nearZeroVar(training,saveMetrics=TRUE)
nsv
```



---

## Spline basis

```{r splines,dependson="dummyVar",cache=TRUE}
library(splines)
bsBasis <- bs(training$age,df=3) 
bsBasis
```

_See also_: ns(),poly()

---

## Fitting curves with splines

```{r ,dependson="splines",fig.height=4,fig.width=4}
lm1 <- lm(wage ~ bsBasis,data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
```


---

## Splines on the test set

```{r ,dependson="splines",fig.height=4,fig.width=4}
predict(bsBasis,age=testing$age)
```


---

## Notes and further reading

* Level 1 feature creation (raw data to covariates)
  * Science is key. Google "feature extraction for [data type]"
  * Err on overcreation of features
* Level 2 feature creation (covariates to new covariates)
  * The function _preProcess_ in _caret_ will handle some preprocessing.
  * Create new covariates if you think they will improve fit
  * Use exploratory analysis on the training set for creating them
  * Be careful about overfitting!
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)
* If you want to fit spline models, use the _gam_ method in the _caret_ package which allows smoothing of multiple variables.
* More on feature creation/data tidying in the Obtaining Data course from the Data Science course track. 



