---
title: "Practicle Machine Learning Week 2 Class Notes"
author: "Yigit Ozan Berk"
date: "9/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The caret package


## The caret R package

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/caret.png height=400>

[http://caret.r-forge.r-project.org/](http://caret.r-forge.r-project.org/)


---

## Caret functionality

* Some preprocessing (cleaning)
  * preProcess()
* Data splitting
  * createDataPartition()
  * createResample()
  * createTimeSlices()
* Training/testing functions
  * train()
  * predict()
* Model comparison
  * confusionMatrix()

---

## Machine learning algorithms in R

* Linear discriminant analysis(MASS package)
* Regression (stats package) 
* Naive Bayes
* Support vector machines
* Classification and regression trees
* Random forests
* Boosting
* etc. 

gbm (gbm package)
mda (mda package)
rpart (rpart package)
Weka (RWeka package)
LogitBoost (caTools package)

---

## Why caret? 

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/predicttable.png height=250>

[add link](??)


--- 

## SPAM Example: Data splitting

```{r loadPackage}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
#75 % to train and 25% to test
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```


--- 

## SPAM Example: Fit a model

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
modelFit
```


--- 

## SPAM Example: Final model

```{r finalModel, dependson="training",cache=TRUE}
modelFit <- train(type ~.,data=training, method="glm")
modelFit$finalModel
```


--- 

## SPAM Example: Prediction

```{r predictions, dependson="training",cache=TRUE}
predictions <- predict(modelFit,newdata=testing)
predictions
```

--- 

## SPAM Example: Confusion Matrix

```{r confusion, dependson="predictions",cache=TRUE}
confusionMatrix(predictions,testing$type)
```

---

## Further information

* Caret tutorials:
  * [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
  * [http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)
* A paper introducing the caret package
  * [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)
  


# Data Slicing



## SPAM Example: Data splitting

```{r loadPackage}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
# 75 % for training set, 25% for test set
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```

---

## SPAM Example: K-fold

```{r kfold,dependson="loadPackage"}
# for cross validation
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=TRUE)
#list = T means it will return each set of imbecies corresponding to a particular fold as a list. 
# returnTrain = T to return the training set
sapply(folds,length)
folds[[1]][1:10] #doesn't change the arrangement of the data
```

---

## SPAM Example: Return test

```{r kfoldtest,dependson="loadPackage"}
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=FALSE)
# to return the test set
sapply(folds,length)
folds[[1]][1:10] #for looking at the id of the sample
```

---

## SPAM Example: Resampling

```{r resample,dependson="loadPackage"}
set.seed(32323)
folds <- createResample(y=spam$type,times=10,
                             list=TRUE)
#for resampling or bootstrapping

sapply(folds,length)
folds[[1]][1:10]
#since we're doing resampling with replacement, you can get the same value multiple times
```

---

## SPAM Example: Time Slices

```{r time,dependson="loadPackage"}
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,
                          horizon=10)
# initialWindow = 20 == window of 20 samples in them
# horizon = 10 == i'm going to predict the next 10 samples after taking the initial window of 20 values
names(folds)
folds$train[[1]]
folds$test[[1]]
```


---

## Further information

* Caret tutorials:
  * [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
  for time slicing
  * [http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)
* A paper introducing the caret package
  * [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)




# Training Options


some of the training control options


## SPAM Example

```{r loadPackage,cache=TRUE}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
modelFit <- train(type ~.,data=training, method="glm")
```

---

## Train options

```{r ,dependson="loadPackage"}
args(train.default)
```
method
preProcess
weights
metric (Accuracy for binary, RMSE for continuous)
trControl = trainControl()


---

## Metric options

__Continous outcomes__:
  * _RMSE_ = Root mean squared error
  * _RSquared_ = $R^2$ from regression models

__Categorical outcomes__:
  * _Accuracy_ = Fraction correct
  * _Kappa_ = A measure of [concordance](http://en.wikipedia.org/wiki/Cohen%27s_kappa)
  
  

--- 

## trainControl

```{r , dependson="loadPackage",cache=TRUE}
args(trainControl)
```
method = "boot" == bootstrap or cross validation
number == number of times to do bootstrapping or cross valiadation
repeats == number of times to repeat cross validation
p = size of the training set 
other parameters for time course data initialWindow for number of time points that will be in the training data
horizon == number of time points that you'll be predicting
returning predictions themselves == savePredictions = T
pre proccessing options = preProcOptions
prediction bounds
seeds
parallelizing computations across multiple cores(for large number of samples.)



--- 

## trainControl resampling
for trainControl function
* _method_
  * _boot_ = bootstrapping
  * _boot632_ = bootstrapping with adjustment
  * _cv_ = cross validation
  * _repeatedcv_ = repeated cross validation
  * _LOOCV_ = leave one out cross validation
* _number_
  * For boot/cross validation
  * Number of subsamples to take
* _repeats_
  * Number of times to repeate subsampling
  * If big this can _slow things down_


---

## Setting the seed

* It is often useful to set an overall seed
* You can also set a seed for each resample
* Seeding each resample is useful for parallel fits
* _seed_ must be a list with
  * Length equal to number of resamples
  * Length of each element equal to number of models fit



--- 



## seed example

```{r seedExample, dependson="loadPackage",cache=TRUE}
set.seed(1235); seeds <- vector(26,mode="list")
for(i in 1:26){seeds[[i]] <- floor(runif(1,0,1e5))}
trControl <- trainControl(seeds=seeds)
```



--- 


## seed example

```{r , dependson="seedExample",cache=TRUE}
modelFit2 <- train(type ~.,data=training, method="glm")
modelFit2
```


--- 

## seed example

```{r , dependson="seedExample",cache=TRUE}
modelFit3 <- train(type ~.,data=training, method="glm")
modelFit3
```
  
  

do caret tutorial and model training and tuning tutorial







# Plotting Predictors




## Example: predicting wages

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/wages.jpg height=350>

Image Credit [http://www.cahs-media.org/the-high-cost-of-low-wages](http://www.cahs-media.org/the-high-cost-of-low-wages)

Data from: [ISLR package](http://cran.r-project.org/web/packages/ISLR) from the book: [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)  



---

## Example: Wage data

```{r loadData,cache=TRUE}
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)
```



---

## Get training/test sets

```{r trainingTest,dependson="loadData",cache=TRUE}
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```


---

## Feature plot (*caret* package)

```{r ,dependson="trainingTest",fig.height=4,fig.width=4}
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
#y is our outcome, featurePlot plots  outcome against each predictor
```


---

## Qplot (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,data=training)
#what is this strange relationship
```


---

## Qplot with color (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,colour=jobclass,data=training)
#check if job class makes a difference
```
most of the up chunk individuals are information based jobs

---

## Add regression smoothers (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qq <- qplot(age,wage,colour=education,data=training)
qq +  geom_smooth(method='lm',formula=y~x)
```


---

## cut2, making factors (*Hmisc* package)


```{r cut2,dependson="trainingTest",fig.height=4,fig.width=6,cache=TRUE}
install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
#let's see if different quantiles have different characteristic
```

---

## Boxplots with cut2


```{r ,dependson="cut2plot",fig.height=4,fig.width=6,cache=TRUE}
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot"))
p1
#let's see wage groups versus age
```

---

### Boxplots with points overlayed


```{r ,dependson="cut2plot",fig.height=4,fig.width=9}
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot","jitter"))
#boxplot and jitter together to see both boxplots and the points 
#to get a better sense of data points in the boxes 
library(gridExtra)
grid.arrange(p1,p2,ncol=2)
```


---

### Tables

```{r ,dependson="cut2",fig.height=4,fig.width=9}
t1 <- table(cutWage,training$jobclass)
t1
prop.table(t1,1)#proportions in each group
#check the data groups
```
more industrial and less information in lower wage jobs
more information and less industrial in higher wage jobs



---

### Density plots

```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(wage,colour=education,data=training,geom="density")
# like histogram
```


---

## Notes and further reading

* Make your plots only in the training set !!!
  * Don't use the test set for exploration!
* Things you should be looking for
  * Imbalance in outcomes/predictors 
  * Outliers (to find out if you're missing any variables)
  * Groups of points not explained by a predictor
  * Skewed variables (you may want to transform if you're using regression models)
* [ggplot2 tutorial](http://rstudio-pubs-static.s3.amazonaws.com/2176_75884214fc524dc0bc2a140573da38bb.html)
* [caret visualizations](http://caret.r-forge.r-project.org/visualizations.html)



# Preprocessing


You plotted variables and found out a very strange predictor or a strange distribution, and you need to deal with it.

## Why preprocess?

```{r loadPackage,cache=TRUE,fig.height=3.5,fig.width=3.5}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
```
almost all are very small but some are much larger.


---

## Why preprocess?

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
mean(training$capitalAve)
sd(training$capitalAve)
```

the data is very skewed and highly variable!

---

## Standardizing

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(trainCapAveS) # mean will be 0
sd(trainCapAveS) # sd will be 1
```
this will reduce a lot of the variability

---

## Standardizing - test set !!!!!!

when we apply a prediction algorithm to the test set, we can only use parameters that we estimated in the training set. 
when we apply the same standardization to the test set, we need to apply the same mean from the training set, and the same standart deviation from the training set!
```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(testCapAveS) #not zero because the mean used was of the training set.
sd(testCapAveS) # not 1 because the mean used was of the training set.
```


---

## Standardizing - _preProcess_ function

```{r preprocess,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
preObj <- preProcess(training[,-58],method=c("center","scale"))#58 is the actual outcome we cared about.
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```


---

## Standardizing - _preProcess_ function

```{r ,dependson="preprocess",cache=TRUE,fig.height=3.5,fig.width=3.5}
testCapAveS <- predict(preObj,testing[,-58])$capitalAve #then use the object created by the training set to preprocess the test set. (like using the mean of training set to center test set.)
mean(testCapAveS)
sd(testCapAveS)
```

---

## Standardizing - _preProcess_ argument

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
#directly passing the preProcess option to train function.
modelFit <- train(type ~.,data=training,
                  preProcess=c("center","scale"),method="glm")
modelFit
```


---

## Standardizing - Box-Cox transforms

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
preObj <- preProcess(training[,-58],method=c("BoxCox"))
#box cox transforms
#set of transformations that take continuous data, and try to make them look like normal data. by estimating a specific set of parameters using maximum likelihood.

trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
```
this is a continuous transform, if you have >0 data it cannot take care of repeated values ( values around 0 usually..).

---

## Standardizing - Imputing data

```{r knn,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
install.packages("RANN")
library(RANN)
set.seed(13343)
# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA
# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
```


---

## Standardizing - Imputing data

```{r ,dependson="knn",cache=TRUE,fig.height=3.5,fig.width=7}
quantile(capAve - capAveTruth)
#difference in values are mostly close to zero
quantile((capAve - capAveTruth)[selectNA])
#only for the ones that were missing
quantile((capAve - capAveTruth)[!selectNA])
```

---

## Notes and further reading

* Training and test must be processed in the same way !!!!
* Test transformations will likely be imperfect
  * Especially if the test/training sets collected at different times
* Careful when transforming factor variables!!!
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)




# Covariate Creation

Covariates == Predictors == Features



## Two levels of covariate creation

**Level 1: From raw data to covariate**

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/covCreation1.png height=200>

**Level 2: Transforming tidy covariates** 

```{r spamData,fig.height=4,fig.width=4}
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2
```


---

## Level 1, Raw data -> covariates

* Depends heavily on application
* The balancing act is summarization vs. information loss
* Examples:
  * Text files: frequency of words, frequency of phrases ([Google ngrams](https://books.google.com/ngrams)), frequency of capital letters.
  * Images: Edges, corners, blobs, ridges ([computer vision feature detection](http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)))
  * Webpages: Number and type of images, position of elements, colors, videos ([A/B Testing](http://en.wikipedia.org/wiki/A/B_testing))
  * People: Height, weight, hair color, sex, country of origin. 
* The more knowledge of the system you have the better the job you will do. 
* When in doubt, err on the side of more features
* Can be automated, but use caution!


---

## Level 2, Tidy covariates -> new covariates

* More necessary for some methods (regression, svms) than for others (classification trees).
* Should be done _only on the training set_
* The best approach is through exploratory analysis (plotting/tables)
* New covariates should be added to data frames



---

## Load example data


```{r loadData,cache=TRUE}
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```


---

## Common covariates to add, dummy variables

__Basic idea - convert factor variables to [indicator variables](http://bit.ly/19ZhWB6)__

dummyVars function easily does this.
separating categorical variables into dummy variables:
```{r dummyVar,dependson="loadData"}
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training)
head(predict(dummies,newdata=training))
```



---

## Removing zero covariates

identify variables with near zero variability, and probably not good predictors.

This function gives the percentage of unique values.

```{r ,dependson="dummyVar"}
nsv <- nearZeroVar(training,saveMetrics=TRUE)
nsv
```
region is %4 unique, it does not give anything to the prediction.


---

## Spline basis

```{r splines,dependson="dummyVar",cache=TRUE}
library(splines)
bsBasis <- bs(training$age,df=3) #3rd degree polynomial
# 1.age, 2. age^2, 3. age^3
#creates a polynomial variable. Here we get a 3rd degree polynomial.
#allows for a curvy model fitting
bsBasis
```

_See also_: ns(),poly()

---

## Fitting curves with splines

```{r ,dependson="splines",fig.height=4,fig.width=4}
lm1 <- lm(wage ~ bsBasis,data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
```


---

## Splines on the test set

after you created your covariate in the training set, you need to create the same covariate exactly the same way in the test set!!!!
```{r ,dependson="splines",fig.height=4,fig.width=4}
predict(bsBasis,age=testing$age) 
```


---

## Notes and further reading

* Level 1 feature creation (raw data to covariates)
  * Science is key. Google "feature extraction for [data type]"!!!!!!! images, voice, categorical variables, vs....
  * Err on overcreation of features. you can always filter them out.
* Level 2 feature creation (covariates to new covariates)
  * The function _preProcess_ in _caret_ will handle some preprocessing.
  * Create new covariates if you think they will improve fit
  * Use exploratory analysis on the training set for creating them
  * Be careful about overfitting! (beware of featurs 'just good for training set' and not good in general)
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)
* If you want to fit spline models, use the _gam_ (for smoothing multiple variables) method in the _caret_ package which allows smoothing of multiple variables.
* More on feature creation/data tidying in the Obtaining Data course from the Data Science course track. 








# Preprocessing with PCA

often there are a lot of variables that are highly correlated(which means they are telling very similar things.), similar to being the almost exact same variable. in this case it's useful to include some kind of summary.



## Correlated predictors

```{r loadPackage,cache=TRUE,fig.height=3.5,fig.width=3.5}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))#58 is the outcome column
#abs value of corr between the predictor variables
diag(M) <- 0
#every variable has a correlation of 1 with itself(because it's itself!) so remove the 1 values by setting the diagonal to 0
which(M > 0.8,arr.ind=T)
#take a look at which of the variables have a corr bigger than .8
```


---

## Correlated predictors

```{r,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
```
they appear in the data almost exactlythe same.

---

## Basic PCA idea

* We might not need every predictor
* A weighted combination of predictors might be better
* We should pick this combination to capture the "most information" possible
* Benefits
  * Reduced number of predictors
  * Reduced noise (due to averaging)


---

## We could rotate the plot

$$ X = 0.71 \times {\rm num 415} + 0.71 \times {\rm num857}$$

$$ Y = 0.71 \times {\rm num 415} - 0.71 \times {\rm num857}$$

```{r,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)
```
most of the variability is happening in the x axis.

adding the two variables to together captures the most of the information in these two variables, so we'll use that.
---

## Related problems

You have multivariate variables $X_1,\ldots,X_n$ so $X_1 = (X_{11},\ldots,X_{1m})$

* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.
* If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.


The first goal is <font color="#330066">statistical</font> and the second goal is <font color="#993300">data compression</font>.

---

## Related solutions - PCA/SVD

__SVD__

If $X$ is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"

$$ X = UDV^T$$

where the columns of $U$ are orthogonal (left singular vectors), the columns of $V$ are orthogonal (right singluar vectors) and $D$ is a diagonal matrix (singular values). 

__PCA__

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.

---

## Principal components in R - prcomp

```{r prcomp,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam) #PCA on the small dataset of the variables on interest.
plot(prComp$x[,1],prComp$x[,2])
#first principle component captures most of the info.
```

---

## Principal components in R - prcomp

```{r ,dependson="prcomp",cache=TRUE,fig.height=3.5,fig.width=3.5}
prComp$rotation
#the rotation matrix.
```


---

## PCA on SPAM data

```{r spamPC,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
#we can do this to more than 2 variables.
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))# pca of the entire dataset in log10 transformation(because some of the data were skewed)
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
#pca1 vs pca2
```


---

## PCA with caret

```{r ,dependson="spamPC",cache=TRUE,fig.height=3.5,fig.width=3.5}
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
```


---

## Preprocessing with PCA

```{r pcaCaret,dependson="spamPC",cache=TRUE,fig.height=3.5,fig.width=3.5}
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train( x = trainPC, y = training$type , method="glm")
```

---

## Preprocessing with PCA

```{r ,dependson="pcaCaret",cache=TRUE,fig.height=3.5,fig.width=3.5}
testPC <- predict(preProc,log10(testing[,-58]+1))
#again, we use the training pca to predict the test dataset.
confusionMatrix(testing$type,predict(modelFit,testPC))
```

---

## Alternative (sets # of PCs)

```{r ,dependson="pcaCaret",cache=TRUE,fig.height=3.5,fig.width=3.5}
modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))
```

---

## Final thoughts on PCs

* Most useful for linear-type models
* Can make it harder to interpret predictors
* Watch out for outliers! !!!
  * Transform first (with logs/Box Cox)
  * Plot predictors to identify problems
* For more info see 
  * Exploratory Data Analysis
  * [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
  
  
  
  
  

# Plotting with Regression





## Key ideas

* Fit a simple regression model
* Plug in new covariates and multiply by the coefficients
* Useful when the linear model is (nearly) correct

__Pros__:
* Easy to implement
* Easy to interpret

__Cons__:
* Often poor performance in nonlinear settings


---

## Example: Old faithful eruptions

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/yellowstone.png height=400>

Image Credit/Copyright Wally Pacholka [http://www.astropics.com/](http://www.astropics.com/)

---

## Example: Old faithful eruptions

```{r faith}
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
                              p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith)
```

---

## Eruption duration versus waiting time

```{r dependson="faith",fig.height=4,fig.width=4}
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
```

---

## Fit a linear model 

$$ ED_i = b_0 + b_1 WT_i + e_i $$

```{r faithlm,dependson="faith",fig.height=4,fig.width=4}
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)
```


---
## Model fit

```{r dependson="faithlm",fig.height=4,fig.width=4}
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lm1$fitted,lwd=3)
```

---

## Predict a new value

$$\hat{ED} = \hat{b}_0 + \hat{b}_1 WT$$

```{r ,dependson="faithlm",fig.height=4,fig.width=4}
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
```

---

## Plot predictions - training and test

```{r ,dependson="faithlm",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)
```

---

## Get training set/test set errors

```{r ,dependson="faithlm",fig.height=4,fig.width=4}
# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
```

---

## Prediction intervals

```{r ,dependson="faithlm",fig.height=4,fig.width=4}
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
ord <- order(testFaith$waiting) #order the values for the test dataset, test waiting times
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",col=c(1,2,2),lty = c(1,1,1), lwd=3)
```


---

## Same process with caret

```{r caretfaith,dependson="faith",fig.height=4,fig.width=4}
modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
summary(modFit$finalModel)
```


---

## Notes and further reading

* Regression models with multiple covariates can be included
* Often useful in combination with other models 
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)



# Predicting with Regression Multiple Covariates







## Example: predicting wages

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/wages.jpg height=350>

Image Credit [http://www.cahs-media.org/the-high-cost-of-low-wages](http://www.cahs-media.org/the-high-cost-of-low-wages)

Data from: [ISLR package](http://cran.r-project.org/web/packages/ISLR) from the book: [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)



---

## Example: Wage data

```{r loadData,cache=TRUE}
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
```



---

### Get training/test sets

```{r trainingTest,dependson="loadData",cache=TRUE}
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
dim(training); dim(testing)
```



---

### Feature plot

```{r ,dependson="trainingTest",fig.height=4,fig.width=4}
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
```
hard to see here, but you can see that for the job class, information group appears to be higher. 
The age variable has a relationship with the outcome..


---

### Plot age versus wage


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,data=training)
```
there appears to be a trend. also, the group of points at the top seems interesting.

---

### Plot age versus wage colour by jobclass


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,colour=jobclass,data=training)
```
here it's more apparent that the must upwards points are mostly blue, which means they are from the information class jobs.

---

### Plot age versus wage colour by education


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,colour=education,data=training)
```
also, education variable seems to be correlated with higher wage, explaining a lot of variation.


---

## Fit a linear model 

$$ ED_i = b_0 + b_1 age + b_2 I(Jobclass_i="Information") + \sum_{k=1}^4 \gamma_k I(education_i= level k) $$

```{r modelFit,dependson="trainingTest", cache=TRUE,fig.height=4,fig.width=4}
modFit<- train(wage ~ age + jobclass + education,
               method = "lm",data=training)
finMod <- modFit$finalModel
print(modFit)
```

Education levels: 1 = HS Grad, 2 = Some College, 3 = College Grad, 4 = Advanced Degree

---

## Diagnostics

```{r,dependson="modelFit",fig.height=5,fig.width=5}
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
#gives residuals vs fitted values and outliers
```


---

## Color by variables not used in the model 

```{r,dependson="modelFit",fig.height=4,fig.width=6}
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
```

---

## Plot by index

```{r,dependson="modelFit",fig.height=5,fig.width=5}
plot(finMod$residuals,pch=19)
#residuals by index - which row of the dataset
#if this picture is not random, it means you're missing a variable. There is a relationship wrt age, time, or some other factor.
```


---

## Predicted versus truth in test set

```{r predictions, dependson="modelFit",fig.height=4,fig.width=6}
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
```

---

## If you want to use all covariates

```{r allCov,dependson="trainingTest",fig.height=4,fig.width=4,warning=FALSE}
modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)
```


---

## Notes and further reading

* Often useful in combination with other models 
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)



Quiz Question 4

Load the Alzheimer’s disease data using the commands:
```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```

Find all the predictor variables in the training set that begin with IL. Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?

```{r}
library(ggplot2)
library(caret)
ncol(training)
```

```{r}
which(sapply(adData,class)=="factor")
```

```{r}
summary(training$diagnosis)
```

```{r}
training$diagnosis = as.numeric(training$diagnosis)
p <- prcomp(training[,grep('^IL',names(training))])
p$rotation[,1:7]
```

```{r}
qplot(1:length(p$sdev),p$sdev / sum(p$sdev))
```

cumulative sum <.90

```{r}
which(cumsum(p$sdev) / sum(p$sdev) <= .9)
```

```{r}
(cumsum(p$sdev) / sum(p$sdev))[8]
```


```{r}
preProc <- preProcess(training[,grep('^IL',names(training))],method="pca",thres=.9)

# See the result here
preProc
```




# Question 5

```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```


Create a training data set consisting of only the predictors with variable names beginning with IL and the diagnosis. Build two predictive models, one using the predictors as they are and one using PCA with principal components explaining 80% of the variance in the predictors. Use method="glm" in the train function.


What is the accuracy of each method in the test set? Which is more accurate?

```{r}
trainSmall <- data.frame(training[,grep('^IL',names(training))],training$diagnosis)
testSmall <- data.frame(testing[,grep('^IL',names(testing))],testing$diagnosis)
preProc <- preProcess(trainSmall[-13],method="pca",thres=.8)
trainPC <- predict(preProc,trainSmall[-13])
testPC <- predict(preProc,testSmall[-13])

New_training <- data.frame(training[,grep('^IL',names(training))],training$diagnosis)
New_testing <- data.frame(testing[,grep('^IL',names(testing))],testing$diagnosis)


set.seed(13)
PCFit <- train(training.diagnosis ~.,
               data = New_training, 
               method ="glm",
               preProc = "pca",
               trControl = trainControl(preProcOptions = list(thresh = 0.8)))

NotPCFit <- train(training.diagnosis~.,data=New_training,method="glm")

PCTestPredict <- predict(PCFit,newdata=New_testing)
NotPCTestPredict <- predict(NotPCFit,newdata=New_testing)

confusionMatrix(PCTestPredict,testSmall$testing.diagnosis)
confusionMatrix(NotPCTestPredict,testSmall$testing.diagnosis)

```














max kuhn
I don't have the data so it is hard to tell. You probably don't want to use the formula method with the data frame name. Try using Segment ~ ., data = trainset[-c(1,2)] instead.

New_training <- data.frame(training[,grep('^IL',names(training))],training$diagnosis)
New_testing <- data.frame(testing[,grep('^IL',names(testing))],testing$diagnosis)


set.seed(13)
PCFit <- train(training.diagnosis ~.,
               data = New_training, 
               method ="glm",
               preProc = "pca",
               trControl = trainControl(preProcOptions = list(thresh = 0.8)))


it will automatically preprocess the data and assemble the features and outcome (and also do this on new data being predicted). The code is a lot cleaner and less subject to issues.
the variation caused by PCA is included in resample (i.e. the PCA loadings are recalculated for every resample as well as the final time on the entire training set). Doing PCA outside of sampling will give the resampling results an optimistic bias although it is hard to say how much).