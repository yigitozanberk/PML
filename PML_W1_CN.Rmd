---
title: "Practicle Machine Learning Week 1"
author: "Yigit Ozan Berk"
date: "9/9/2019"
output: html_document
---

Chris Volinsky - team that won the netflix prize - reducing the error when netflix is predicting which movies the customer wants most

Kaggle hosts competitions.

# Class Syllabus
Prediction study design

In sample and out of sample errors

Overfitting

Receiver Operating Characteristic (ROC) curves

The caret package in R

Preprocessing and feature creation

Prediction with regression

Prediction with decision trees

Prediction with random forests

Boosting

Prediction blending


# Week 1

- Study design 
training vs. test sets
- Conceptual issues - out of sample error, ROC curves
- Practicle implementation - the caret package

```{r}
install.packages("caret")
```

## A useful (if a bit advanced) book

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/elemlearn.png height=350>


[The elements of statistical learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)

---

## A useful package

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/caret.png height=350>


[http://caret.r-forge.r-project.org/](http://caret.r-forge.r-project.org/)


---

## Machine learning (more advanced material)

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/machinelearning.png height=350>


[https://www.coursera.org/course/ml](https://www.coursera.org/course/ml)


---

## Even more resources

* [List of machine learning resources on Quora](http://www.quora.com/Machine-Learning/What-are-some-good-resources-for-learning-about-machine-learning-Why)
* [List of machine learning resources from Science](http://www.sciencemag.org/site/feature/data/compsci/machine_learning.xhtml)
* [Advanced notes from MIT open courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)
* [Advanced notes from CMU](http://www.stat.cmu.edu/~cshalizi/350/)
* [Kaggle - machine learning competitions](http://www.kaggle.com/)

# What is prediction?

question > input data > features > algorithm > parameters > evaluation

```{r}
install.packages("kernlab")
library(kernlab)
```

count the number of times the email says "you". Divide it to the total number of words. Now you have a quantitative variable of the percentage of words that are "you" in an email.


```{r}
data(spam)
head(spam)
```

```{r}
plot(density(spam$your[spam$type == "nonspam"]),
     col = "blue", main = "", xlab = "Frequency of 'your")
lines(density(spam$your[spam$type == "spam"]), col = "red")
```

the number of "your" is much higher in spam emails.

find a cutoff value of frequency of "your"

frequency of 'your' > spam

```{r}
plot(density(spam$your[spam$type == "nonspam"]),
     col = "blue", main = "", xlab = "Frequency of 'your")
lines(density(spam$your[spam$type == "spam"]), col = "red")
abline(v = 0.5, col = "black")
```

```{r}
prediction <- ifelse(spam$your > 0.5, "spam", "nonspam")
#if the freq is above 0.5 mark as spam.. versus the real values table.
table(prediction, spam$type)/length(spam$type)
```

accuracy = 0.459 + 0.292 = 0.751.

our algorithm is 75% accurate in this particular case.

## Relative order of importance

question > data > features > algorithms

(image and voice data can require a certain kind of prediction algorithms.)

the combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
- tukey

garbage in = garbage out

if you have bad data, no matter how good your algorithm is you'll get bad results

question -> *input data* -> features -> algorithm -> parameters -> evaluation

1. may be easy (movie ratings -> new movie ratings)
2. may be harder (gene expression data -> disease)
3. depends on what is a 'good prediction'.
4. often more data > better models
5. the most important step - collecting the right data.

question -> input data -> *features* -> algorithm -> parameters -> evaluation
features matter.

Properties of good features
- lead to data compression(without losing meaningful details while compressing dimensions)
- retain relevant information
- are created based on expert application knowledge

common mistakes
- trying to automate feature selection(blackbox predictions can be very useful and accurate, but they can change in a dime if you don't know *how* those features predict outcomes)
- not paying attention to data-specific quirks(outliers, specific weird features)
- throwing away information unnecessarily

feature selection automation is a field itself(image processing) - deep learning

question -> input data -> features -> *algorithm* -> parameters -> evaluation

algorithms matter less than you think. so it seems like using the same method over and over again did make the prediction error worse but not incredibly worse.
Using a very sensible approach will get you a very large weight of solving the problem. getting the absolute best method can *improve* that result.

Issues to Consider:
the "best" machine learning method
- interpretable
- simple
- accurate
- fast(to train and test)
- scalable (easy to apply to a large data set. fast or parallelizable)

## Prediction is about accuracy tradeoffs

- interpretability versus accuracy
- speed versus accuracy
- simplicity versus accuracy
- scalability versus accuracy

interpretability matters
'if' total cholesterol >= 160 'and' smoke 'then' 10 year CHD risk >= 5%
'else if' smoke 'and' systolic blood pressure >= 140 'then' 10 year CHD risk >= 5%
'else' 10 year CHD risk < 5%

Scalability matters
the netflix challenge algorithm did not scale well. long time to compute the huge datasets of netflix. accuracy isn't only the best and only decision maker.

# in and out of sample errors

in sample error: the error rate you get on the same data set you used to build your predictor. sometimes called resubstitution error.

out of sample error : the error rate you get on a new data set. sometimes called generalization error.

Key ideas:
1. out of sample error is what you care about
2. in sample error < out of sample error
3. the reason is overfitting.
- matching your algorithm to the data you have.

sometimes your machine learning algorithm will tune itself to the noise in your training data set.

```{r}
library(kernlab); data(spam); set.seed(333)
smallSpam <- spam[sample(dim(spam)[1], size = 10), ]
#10 messages
spamLabel <- (smallSpam$type == "spam")*1 + 1
#whether you see a lot of capital letters
plot(smallSpam$capitalAve, col = spamLabel)
```

some of the spam messages have a lot more capital letters.


so you might want to build a predictor based on the average number of capital letters as to whether you are a spam message or you're a ham message.

### rule1
capitalAve > 2.7 = "spam"
capitalAve <2.40 = "nonspam"
capitalAve between 2.40 and 2.45 = "spam"
capitalAve between 2.45 and 2.7 = "nonspam"

we can do the last two lines to perfectly tune the algorithm to predict those two spam values in the training set as well. This makes training set accuracy perfect

```{r}
rule1 <- function(x) {
        prediction <- rep(NA, length(x))
        prediction[x > 2.7] <- "spam"
        prediction[x <2.40] <- "nonspam"
        prediction[(x >= 2.40 & x <= 2.45)] <- "spam"
        prediction[(x > 2.45 & x <= 2.70)] <- "nonspam"
        return(prediction)
}
table(rule1(smallSpam$capitalAve), smallSpam$type)
```

### rule2

capitalAve > 2.80 = "spam"
capitalAve <= 2.80 = "nonspam"

```{r}
rule2 <- function(x) {
        prediction <- rep(NA, length(x))
        prediction[x > 2.80] <- "spam"
        prediction[x <= 2.80] <- "nonspam"
        return(prediction)
}
table(rule2(smallSpam$capitalAve), smallSpam$type)
```

apply to all data

```{r}
table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve),spam$type)
mean(rule1(spam$capitalAve)==spam$type)
mean(rule2(spam$capitalAve)==spam$type)
```



## Look at accuracy

```{r, dependson="loadData"}
sum(rule1(spam$capitalAve)==spam$type)
sum(rule2(spam$capitalAve)==spam$type)
#number of times we were right
# 30 more right answers for the simplified rule.
```


---

## What's going on? 

<center><rt> Overfitting </rt></center>

* Data have two parts
  * Signal
  * Noise
* The goal of a predictor is to find signal (and ignore the noise) !!!!!!!!!!!
* You can always design a perfect in-sample predictor
* You capture both signal + noise when you do that
* Predictor won't perform as well on new samples

[http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting)




# Prediction Study Design

*How to minimize the problems that can be cause by in sample vs out of sample errors.*

1. Define your error rate(multiple error rates you can choose)
2. Split data into:
- training, testing, validation(optional)
3. on the training set pick features
- use cross validation
4. on the training set pick prediction function
- use cross validation
5. if no validation
- apply 1x to test set(if you apply more than once, you are using the test set as the training set too)(apply only the best prediction model)
6. if validation(if you have validation sets, you can apply more than one prediction alternatives to the test set)
- apply to test set and refine
- apply 1x to validation(apply only the best prediction model)



## Know the benchmarks

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/benchmark.png height=400>

[http://www.heritagehealthprize.com/c/hhp/leaderboard](http://www.heritagehealthprize.com/c/hhp/leaderboard)


---

## Study design

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/studyDesign.png height=400>


[http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf](http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf)

---

## Used by the professionals

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/kagglefront.png height=400>

[http://www.kaggle.com/](http://www.kaggle.com/)

---

## Avoid small sample sizes

* Suppose you are predicting a binary outcome 
  * Diseased/healthy
  * Click on add/not click on add 
* One classifier is flipping a coin
* Probability of perfect classification is approximately:
  * $\left(\frac{1}{2}\right)^{sample \; size}$
  * $n = 1$ flipping coin 50% chance of 100% accuracy
  * $n = 2$ flipping coin 25% chance of 100% accuracy
  * $n = 10$ flipping coin 0.10% chance of 100% accuracy

---

## Rules of thumb for prediction study design

* If you have a large sample size
  * 60% training
  * 20% test
  * 20% validation
* If you have a medium sample size
  * 60% training
  * 40% testing
* If you have a small sample size
  * Do cross validation
  * Report caveat of small sample size

---

## Some principles to remember

* Set the test/validation set aside and _don't look at it_
* In general _randomly_ sample training and test
* Your data sets must reflect structure of the problem
  * If predictions evolve with time split train/test in time chunks (called[backtesting](http://en.wikipedia.org/wiki/Backtesting) in finance)
* All subsets should reflect as much diversity as possible
  * Random assignment does this
  * You can also try to balance by features - but this is tricky


# Types of Errors




## Basic terms

In general, __Positive__ = identified and __negative__ = rejected. Therefore:

__True positive__ = correctly identified

__False positive__ = incorrectly identified

__True negative__ = correctly rejected

__False negative__ = incorrectly rejected

_Medical testing example_:

__True positive__ = Sick people correctly diagnosed as sick

__False positive__= Healthy people incorrectly identified as sick

__True negative__ = Healthy people correctly identified as healthy

__False negative__ = Sick people incorrectly identified as healthy.

[http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

## Define your error rate

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/sensspec.png height=400>


[http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

## Why your choice matters

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/sensspecex.png height=400>

[http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity)


---

## For continuous data

__Mean squared error (MSE)__:

$$\frac{1}{n} \sum_{i=1}^n (Prediction_i - Truth_i)^2$$

__Root mean squared error (RMSE)__:

$$\sqrt{\frac{1}{n} \sum_{i=1}^n(Prediction_i - Truth_i)^2}$$

---

## Common error measures

1. Mean squared error (or root mean squared error)
  * Continuous data, sensitive to outliers
2. Median absolute deviation 
  * Continuous data, often more robust
3. Sensitivity (recall)
  * If you want few missed positives
4. Specificity
  * If you want few negatives called positives
5. Accuracy
  * Weights false positives/negatives equally
6. Concordance
  * One example is [kappa](http://en.wikipedia.org/wiki/Cohen%27s_kappa)
  for multiclass measurements
5. Predictive value of a positive (precision)
  * When you are screeing and prevelance is low
  
  
# Receiver Operating Characteristic


## Why a curve?

* In binary classification you are predicting one of two categories
  * Alive/dead
  * Click on ad/don't click
* But your predictions are often quantitative
  * Probability of being alive
  * Prediction on a scale from 1 to 10
* The _cutoff_  you choose gives different results

---

## ROC curves

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/roc1.png height=400>

[http://en.wikipedia.org/wiki/Receiver_operating_characteristic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)

---

## An example

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/roc2.png height=400>

[http://en.wikipedia.org/wiki/Receiver_operating_characteristic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)

---

## Area under the curve

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/roc1.png height=200>

* AUC = 0.5: random guessing
* AUC = 1: perfect classifer
* In general AUC of above 0.8 considered "good"

[http://en.wikipedia.org/wiki/Receiver_operating_characteristic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)

---

## What is good?

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/roc3.png height=400>

[http://en.wikipedia.org/wiki/Receiver_operating_characteristic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)


# Cross Validation

## Study design

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/studyDesign.png height=400>


[http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf](http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf)

---

## Key idea

1. Accuracy on the training set (resubstitution accuracy) is optimistic
2. A better estimate comes from an independent set (test set accuracy)
3. But we can't use the test set when building the model or it becomes part of the training set
4. So we estimate the test set accuracy with the training set. 
!!!!!!!


---

## Cross-validation

_Approach_:

1. Use the training set

2. Split it into training/test sets 

3. Build a model on the training set

4. Evaluate on the test set

5. Repeat and average the estimated errors

_Used for_:

1. Picking variables to include in a model

2. Picking the type of prediction function to use

3. Picking the parameters in the prediction function

4. Comparing different predictors

---

## Random subsampling


<img class=center src=../../assets/img/08_PredictionAndMachineLearning/random.png height=400>


---

## K-fold


<img class=center src=../../assets/img/08_PredictionAndMachineLearning/kfold.png height=400>

---

## Leave one out

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/loocv.png height=400>

---

## Considerations

* For time series data data must be used in "chunks"
* For k-fold cross validation
  * Larger k = less bias, more variance(will depend a lot on which subsets you take)
  * Smaller k = more bias, less variance(we won't get as good an estimate, but there'll be less variance)
* Random sampling must be done _without replacement_
* Random sampling with replacement is the _bootstrap_
  * Underestimates of the error(if you get one value right, you'll automatically get the other equals right as well)
  * Can be corrected, but it is complicated ([0.632 Bootstrap](http://www.jstor.org/discover/10.2307/2965703?uid=2&uid=4&sid=21103054448997))
* If you cross-validate to pick predictors estimate you must estimate errors on independent data. 






# What data to use


## A succcessful predictor

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/fivethirtyeight.png height=400>

[fivethirtyeight.com](fivethirtyeight.com)

---

## Polling data

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/gallup.png height=400>

[http://www.gallup.com/](http://www.gallup.com/)

---

## Weighting the data

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/538.png height=400>

[http://www.fivethirtyeight.com/2010/06/pollster-ratings-v40-methodology.html](http://www.fivethirtyeight.com/2010/06/pollster-ratings-v40-methodology.html)

---

## Key idea

<center>To predict X use data related to X</center>


---

## Key idea

<center>To predict player performance use data about player performance</center>

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/moneyball.jpg height=400>

---

## Key idea

<center>To predict movie preferences use data about movie preferences</center>

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/netflix.png height=400>

---

## Key idea

<center>To predict hospitalizations use data about hospitalizations</center>

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/heritage.png height=400>

---

## Not a hard rule

<center>To predict flu outbreaks use Google searches</center>

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/flutrends.png height=400>

[http://www.google.org/flutrends/](http://www.google.org/flutrends/)

---

## Looser connection = harder prediction

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/oncotype.png height=300>

---

## Data properties matter

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/fluproblems.jpg height=400>

---

## Unrelated data is the most common mistake

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/choc.png height=400>

[http://www.nejm.org/doi/full/10.1056/NEJMon1211064](http://www.nejm.org/doi/full/10.1056/NEJMon1211064)


