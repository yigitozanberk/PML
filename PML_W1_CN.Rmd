---
title: "Practicle Machine Learning Week 1"
author: "Yigit Ozan Berk"
date: "9/9/2019"
output: html_document
---

Chris Volinsky - team that won the netflix prize - reducing the error when netflix is predicting which movies the customer wants most

Kaggle hosts competitions.

# Class Syllabus
Prediction study design

In sample and out of sample errors

Overfitting

Receiver Operating Characteristic (ROC) curves

The caret package in R

Preprocessing and feature creation

Prediction with regression

Prediction with decision trees

Prediction with random forests

Boosting

Prediction blending


# Week 1

- Study design 
training vs. test sets
- Conceptual issues - out of sample error, ROC curves
- Practicle implementation - the caret package

```{r}
install.packages("caret")
```

## A useful (if a bit advanced) book

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/elemlearn.png height=350>


[The elements of statistical learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)

---

## A useful package

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/caret.png height=350>


[http://caret.r-forge.r-project.org/](http://caret.r-forge.r-project.org/)


---

## Machine learning (more advanced material)

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/machinelearning.png height=350>


[https://www.coursera.org/course/ml](https://www.coursera.org/course/ml)


---

## Even more resources

* [List of machine learning resources on Quora](http://www.quora.com/Machine-Learning/What-are-some-good-resources-for-learning-about-machine-learning-Why)
* [List of machine learning resources from Science](http://www.sciencemag.org/site/feature/data/compsci/machine_learning.xhtml)
* [Advanced notes from MIT open courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)
* [Advanced notes from CMU](http://www.stat.cmu.edu/~cshalizi/350/)
* [Kaggle - machine learning competitions](http://www.kaggle.com/)

# What is prediction?

question > input data > features > algorithm > parameters > evaluation

```{r}
install.packages("kernlab")
library(kernlab)
```

count the number of times the email says "you". Divide it to the total number of words. Now you have a quantitative variable of the percentage of words that are "you" in an email.


```{r}
data(spam)
head(spam)
```

```{r}
plot(density(spam$your[spam$type == "nonspam"]),
     col = "blue", main = "", xlab = "Frequency of 'your")
lines(density(spam$your[spam$type == "spam"]), col = "red")
```

the number of "your" is much higher in spam emails.

find a cutoff value of frequency of "your"

frequency of 'your' > spam

```{r}
plot(density(spam$your[spam$type == "nonspam"]),
     col = "blue", main = "", xlab = "Frequency of 'your")
lines(density(spam$your[spam$type == "spam"]), col = "red")
abline(v = 0.5, col = "black")
```

```{r}
prediction <- ifelse(spam$your > 0.5, "spam", "nonspam")
#if the freq is above 0.5 mark as spam.. versus the real values table.
table(prediction, spam$type)/length(spam$type)
```

accuracy = 0.459 + 0.292 = 0.751.

our algorithm is 75% accurate in this particular case.

## Relative order of importance

question > data > features > algorithms

(image and voice data can require a certain kind of prediction algorithms.)

the combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
- tukey

garbage in = garbage out

if you have bad data, no matter how good your algorithm is you'll get bad results

question -> *input data* -> features -> algorithm -> parameters -> evaluation

1. may be easy (movie ratings -> new movie ratings)
2. may be harder (gene expression data -> disease)
3. depends on what is a 'good prediction'.
4. often more data > better models
5. the most important step - collecting the right data.

question -> input data -> *features* -> algorithm -> parameters -> evaluation
features matter.

Properties of good features
- lead to data compression(without losing meaningful details while compressing dimensions)
- retain relevant information
- are created based on expert application knowledge

common mistakes
- trying to automate feature selection(blackbox predictions can be very useful and accurate, but they can change in a dime if you don't know *how* those features predict outcomes)
- not paying attention to data-specific quirks(outliers, specific weird features)
- throwing away information unnecessarily

feature selection automation is a field itself(image processing) - deep learning

question -> input data -> features -> *algorithm* -> parameters -> evaluation

algorithms matter less than you think. so it seems like using the same method over and over again did make the prediction error worse but not incredibly worse.
Using a very sensible approach will get you a very large weight of solving the problem. getting the absolute best method can *improve* that result.

Issues to Consider:
the "best" machine learning method
- interpretable
- simple
- accurate
- fast(to train and test)
- scalable (easy to apply to a large data set. fast or parallelizable)

## Prediction is about accuracy tradeoffs

- interpretability versus accuracy
- speed versus accuracy
- simplicity versus accuracy
- scalability versus accuracy

interpretability matters
'if' total cholesterol >= 160 'and' smoke 'then' 10 year CHD risk >= 5%
'else if' smoke 'and' systolic blood pressure >= 140 'then' 10 year CHD risk >= 5%
'else' 10 year CHD risk < 5%

Scalability matters
the netflix challenge algorithm did not scale well. long time to compute the huge datasets of netflix. accuracy isn't only the best and only decision maker.

# in and out of sample errors

in sample error: the error rate you get on the same data set you used to build your predictor. sometimes called resubstitution error.

out of sample error : the error rate you get on a new data set. sometimes called generalization error.

Key ideas:
1. out of sample error is what you care about
2. in sample error < out of sample error
3. the reason is overfitting.
- matching your algorithm to the data you have.

sometimes your machine learning algorithm will tune itself to the noise in your training data set.

```{r}
library(kernlab); data(spam); set.seed(333)
smallSpam <- spam[sample(dim(spam)[1], size = 10), ]
#10 messages
spamLabel <- (smallSpam$type == "spam")*1 + 1
#whether you see a lot of capital letters
plot(smallSpam$capitalAve, col = spamLabel)
```

some of the spam messages have a lot more capital letters.


so you might want to build a predictor based on the average number of capital letters as to whether you are a spam message or you're a ham message.

### rule1
capitalAve > 2.7 = "spam"
capitalAve <2.40 = "nonspam"
capitalAve between 2.40 and 2.45 = "spam"
capitalAve between 2.45 and 2.7 = "nonspam"

we can do the last two lines to perfectly tune the algorithm to predict those two spam values in the training set as well. This makes training set accuracy perfect

```{r}
rule1 <- function(x) {
        prediction <- rep(NA, length(x))
        prediction[x > 2.7] <- "spam"
        prediction[x <2.40] <- "nonspam"
        prediction[(x >= 2.40 & x <= 2.45)] <- "spam"
        prediction[(x > 2.45 & x <= 2.70)] <- "nonspam"
        return(prediction)
}
table(rule1(smallSpam$capitalAve), smallSpam$type)
```

### rule2

capitalAve > 2.80 = "spam"
capitalAve <= 2.80 = "nonspam"

```{r}
rule2 <- function(x) {
        prediction <- rep(NA, length(x))
        prediction[x > 2.80] <- "spam"
        prediction[x <= 2.80] <- "nonspam"
        return(prediction)
}
table(rule2(smallSpam$capitalAve), smallSpam$type)
```

apply to all data

```{r}
table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve),spam$type)
mean(rule1(spam$capitalAve)==spam$type)
mean(rule2(spam$capitalAve)==spam$type)
```



## Look at accuracy

```{r, dependson="loadData"}
sum(rule1(spam$capitalAve)==spam$type)
sum(rule2(spam$capitalAve)==spam$type)
#number of times we were right
# 30 more right answers for the simplified rule.
```


---

## What's going on? 

<center><rt> Overfitting </rt></center>

* Data have two parts
  * Signal
  * Noise
* The goal of a predictor is to find signal (and ignore the noise) !!!!!!!!!!!
* You can always design a perfect in-sample predictor
* You capture both signal + noise when you do that
* Predictor won't perform as well on new samples

[http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting)


## Prediction Study Design

*How to minimize the problems that can be cause by in sample vs out of sample errors.*

1. Define your error rate(multiple error rates you can choose)
2. Split data into:
- training, testing, validation(optional)
3. on the training set pick features
- use cross validation
4. on the training set pick prediction function
- use cross validation
5. if no validation
- apply 1x to test set(if you apply more than once, you are using the test set as the training set too)(apply only the best prediction model)
6. if validation(if you have validation sets, you can apply more than one prediction alternatives to the test set)
- apply to test set and refine
- apply 1x to validation(apply only the best prediction model)





