---
title: "Practicle Machine Learning Week 3 Class Notes"
author: "Yigit Ozan Berk"
date: "9/16/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Predicting with trees
2. Bagging
3. Random Forests
4. Boosting
5. Model Based Prediction
6. Quiz

# Predicting with Trees



## Key ideas

* Iteratively split variables into groups
* Split where maximally predictive
* Evaluate "homogeneity" within each branch!!(split again if necessary)
* Fitting multiple trees often works better (forests)

__Pros__:
* Easy to implement
* Easy to interpret
* Better performance in nonlinear settings!!!!!(compared to linear models)

__Cons__:
* Without pruning/cross-validation can lead to overfitting!!!
* Harder to estimate uncertainty
* Results may be variable


---

## Example Tree

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/obamaTree.png height=450>

[http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg](http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)

---

## Basic algorithm

1. Start with all variables in one group
2. Find the variable/split that best separates the outcomes
3. Divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

---

## Measures of impurity


$$\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\; in \; Leaf \; m}\mathbb{1}(y_i = k)$$


__Misclassification Error__: 
$$ 1 - \hat{p}_{mk(m)}$$

__Gini index__:
$$ \sum_{k \neq k'} \hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) $$

__Cross-entropy or deviance__:

$$ -\sum_{k=1}^K \hat{p}_{mk} \ln\hat{p}_{mk} $$


---

## Example: Iris Data

```{r iris, cache=TRUE}
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
```
we have 4 predictors, we are trying to predict species

---

## Create training and test sets

```{r trainingTest, dependson="iris",cache=TRUE}
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```


---

## Iris petal widths/sepal width

```{r, dependson="trainingTest",fig.height=4,fig.width=6}
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
```
3 very distinct clusters.

can be very challenging for linear models. but not trees

---

## Iris petal widths/sepal width

```{r createTree, dependson="trainingTest", cache=TRUE}
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
#rpart for classification trees
print(modFit$finalModel)
```

---

## Plot tree

```{r, dependson="createTree", fig.height=4.5, fig.width=4.5}
plot(modFit$finalModel, uniform=TRUE, 
      main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
```


---

## Prettier plots

```{r, dependson="createTree", fig.height=4.5, fig.width=4.5}
library(rattle)
fancyRpartPlot(modFit$finalModel)
#fancier trees
```

---

## Predicting new values

```{r newdata, dependson="createTree", fig.height=4.5, fig.width=4.5, cache=TRUE}
predict(modFit,newdata=testing)
```
classification tree was created to predict a type of species, so predict gives out a list of classes.

---

## Notes and further resources

* Classification trees are non-linear models
  * They use interactions between variables!!!!if you have a large number of classes, the models can overfit a little bit.
  * Data transformations may be less important (monotone transformations)(that doesn't change the order of the variable, but maybe makes them larger)
  * Trees can also be used for regression problems (continuous outcome)(you can use RMSE for classification trees.)
* Note that there are multiple tree building options
in R both in the caret package - [party](http://cran.r-project.org/web/packages/party/index.html), [rpart](http://cran.r-project.org/web/packages/rpart/index.html) and out of the caret package - [tree](http://cran.r-project.org/web/packages/tree/index.html)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)









# Bagging

Sometimes when you have fit complicated models, sometimes if you average those models together, you get a smoother model fit that gives you a better balance between potential bias in your fit, and variance in your fit.


## Bootstrap aggregating (bagging)

__Basic idea__: 

1. Resample cases and recalculate predictions
2. Average or majority vote

__Notes__:

* Similar bias (to fitting any one of the individual models you have fit)
* Reduced variance (because you have averaged a bunch of predictors together)
* More useful for non-linear functions


---

## Ozone data

```{r ozoneData, cache=TRUE}
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]#order by the outcome
head(ozone)
```
[http://en.wikipedia.org/wiki/Bootstrap_aggregating](http://en.wikipedia.org/wiki/Bootstrap_aggregating)

predict temperature as a function of ozone

---

## Bagged loess

```{r baggedOzone, dependson="ozoneData",cache=TRUE}
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
        #resample the dataset for 10 different times.
  ss <- sample(1:dim(ozone)[1],replace=T)
  #each time with replacement.
  ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
  #create a new dataset ozone0.- the reampled data set for that particular element of the loop. that's just the subset of the data set corresponding to our ramdom sample
  #reorder everytime with the ozone variable
  loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
  #fit a loess curve each time. use the resampled data set. and span is a measure of how smooth that vector will be.
  ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
  #prediction of the loess curve.
}
#I've resampled the dataset 10 different times. Fit a smooth curve 10 different times. then i will average those values
```
 


---

## Bagged loess

```{r, dependson="baggedOzone",fig.height=4.5,fig.width=4.5}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
#red line is the averaged loess curve. grey lines are the loess curve of each resampled set.
```

## bagging in caret

some models perform bagging for you, in train() function consider method options:

- bagEarth
- treebag
- bagFDA

alternatively you can bag any model you choose using the bag() function


```{r}
library(party)
predictors = data.frame(ozone = ozone$ozone)
temperature = ozone$temperature
#take you predictor variable and put it in a data frame
#take the outcome variable
treebag = bag(predictors, temperature, B = 10,#no of replications
              bagControl = bagControl(fit = ctreeBag$fit,
              #how i'm going to fit the model
                                      #~call to train function
                                      predict = ctreeBag$pred,
                                      #~call to the predict function from a trained model
                                      aggregate = ctreeBag$aggregate))
                                        #average the results
```
www.inside-r.org/packages/cran/caret/docs/nbBag

```{r}
plot(ozone$ozone, temperature, col = "lightgrey", pch = 19)
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch = 19, col = "red")
points(ozone$ozone, predict(treebag, predictors), pch = 19, col = "blue")
```
grey are the actual observed values. 
red dots represent the fit from a single conditional regression tree
it doesn't capture the trand at the lower left. - flat lines
blue fit is the fit from the bagged regression


```{r}
ctreeBag$fit
```
it takes the data.frame and the outcome, uses the ctree function to create a conditional tree

```{r}
ctreeBag$pred
```
prediction takes in the object from the ctree model fit, and the dataset - to get a new prediction

calculates a probability matrix

```{r}
ctreeBag$aggregate
```
gets the prediction from every single model fits
takes the median of every value.
---

## Notes and further resources

__Notes__:

* Bagging is most useful for nonlinear models!!!!!!!widely used
* Often used with trees - an extension is random forests!!
* Several models use bagging in caret's _train_ function

__Further resources__:

* [Bagging](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
* [Bagging and boosting](http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)






# Random Forests

~ an extension of bagging





## Random forests

1. Bootstrap samples
2. At each split, bootstrap variables!!! (only a subset of the variables is considered in each split)
3. Grow multiple trees and vote

__Pros__:

1. Accuracy!

__Cons__:

1. Speed(it has to build a large number of trees)
2. Interpretability
3. Overfitting(hard to comprehend which trees are causing overfitting)


---

## Random forests

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/forests.png height=400>

[http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf](http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf)


---

## Iris data

```{r iris, cache=TRUE}
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```


---

## Random forests

```{r forestIris, dependson="irisData",fig.height=4,fig.width=4,cache=TRUE}
library(caret)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
#prox = T produces a little bit of extra information
#outcome species, and other variables as potential predictors
modFit
```
mtry = the number of repeated trees that it's going to build
---

## Getting a single tree

```{r , dependson="forestIris",fig.height=4,fig.width=4}
library(randomForest)
getTree(modFit$finalModel,k=2)
```
----------
---

## Class "centers"

```{r centers, dependson="forestIris",fig.height=4,fig.width=4}
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
#centers for the predicted values
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
#centers dataset and species dataset
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
#the pedal width and pedal length points
```

---

## Predicting new values

```{r predForest, dependson="centers",fig.height=4,fig.width=4,cache=TRUE}
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)

```
we missed 2 values with random forest model, but still very accurate
---

## Predicting new values

```{r, dependson="predForest",fig.height=4,fig.width=4}
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
#the two misclassed values are at the edge
```

---

## Notes and further resources

__Notes__:

* Random forests are usually one of the two top
performing algorithms along with boosting in prediction contests.!!!!!
* Random forests are difficult to interpret but often very accurate. 
* Care should be taken to avoid overfitting (see [rfcv](http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) funtion)


__Further resources__:

* [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) -- inventor!!!
* [Random forest Wikipedia](http://en.wikipedia.org/wiki/Random_forest)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)








# Boosting


random forst and boosting is the two most accurately used.





## Basic idea

1. Take lots of (possibly) weak predictors
2. Weight them and add them up
3. Get a stronger predictor


---

## Basic idea behind boosting

1. Start with a set of classifiers $h_1,\ldots,h_k$
  * Examples: All possible trees, all possible regression models, all possible cutoffs.
2. Create a classifier that combines classification functions:
$f(x) = \rm{sgn}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$
  * Goal is to minimize error (on training set)
  * Iterative, select one $h$ at each step
  * Calculate weights based on errors
  * Upweight missed classifications and select next $h$
  
[Adaboost on Wikipedia](http://en.wikipedia.org/wiki/AdaBoost) most famous boosting algorithm

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Simple example

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/ada1.png height=450>

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Round 1: adaboost

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/adar1.png height=450>

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Round 2 & 3

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/ada2.png height=450>

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)


---

## Completed classifier

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/ada3.png height=450>

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Boosting in R 

* Boosting can be used with any subset of classifiers
* One large subclass is [gradient boosting](http://en.wikipedia.org/wiki/Gradient_boosting)!!!!!!
* R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules.
  * [gbm](http://cran.r-project.org/web/packages/gbm/index.html) - boosting with trees.
  * [mboost](http://cran.r-project.org/web/packages/mboost/index.html) - model based boosting
  * [ada](http://cran.r-project.org/web/packages/ada/index.html) - statistical boosting based on [additive logistic regression](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223)
  * [gamBoost](http://cran.r-project.org/web/packages/GAMBoost/index.html) for boosting generalized additive models
* Most of these are available in the caret package 

!!!!!!!!!!!

---

## Wage example

```{r wage, cache=TRUE}
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```


---

## Fit the model

```{r, dependson="wage", cache=TRUE}
install.packages("gbm")
library(gbm)
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
#wage as a function of all variables.
#gbm does boosting with trees
#verbose = F because it will produce a lot of outcome
print(modFit)
```

---

## Plot the results

```{r, dependson="wage", fig.height=4,fig.width=4}
qplot(predict(modFit,testing),wage,data=testing)
#predicting results of the testing set
```
reasonable good prediction, although it seems there is reasonably more variability here


---

## Notes and further reading

* A couple of nice tutorials for boosting
  * Freund and Shapire - [http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf)
  * Ron Meir- [http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)
* Boosting, random forests, and model ensembling are the most common tools that win Kaggle and other prediction contests. 
  * [http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)
  * [https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)
  
  random forest and boosting combination in the link above.
  
  
  
  



# Model Based Prediction






## Basic idea

1. Assume the data follow a probabilistic model
2. Use Bayes' theorem to identify optimal classifiers

__Pros:__

* Can take advantage of structure of the data
* May be computationally convenient
* Are reasonably accurate on real problems

__Cons:__

* Make additional assumptions about the data
* When the model is incorrect you may get reduced accuracy

---

## Model based approach


1. Our goal is to build parametric model for conditional distribution $P(Y = k | X = x)$

2. A typical approach is to apply [Bayes theorem](http://en.wikipedia.org/wiki/Bayes'_theorem):
$$ Pr(Y = k | X=x) = \frac{Pr(X=x|Y=k)Pr(Y=k)}{\sum_{\ell=1}^K Pr(X=x |Y = \ell) Pr(Y=\ell)}$$
$$Pr(Y = k | X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}$$

3. Typically prior probabilities $\pi_k$ are set in advance.

4. A common choice for $f_k(x) = \frac{1}{\sigma_k \sqrt{2 \pi}}e^{-\frac{(x-\mu_k)^2}{\sigma_k^2}}$, a Gaussian distribution

5. Estimate the parameters ($\mu_k$,$\sigma_k^2$) from the data.

6. Classify to the class with the highest value of $P(Y = k | X = x)$

---

## Classifying using the model

A range of models use this approach

* Linear discriminant analysis assumes $f_k(x)$ is multivariate Gaussian with same covariances (lda)
* Quadratic discrimant analysis assumes $f_k(x)$ is multivariate Gaussian with different covariances
* [Model based prediction](http://www.stat.washington.edu/mclust/) assumes more complicated versions for the covariance matrix 
* Naive Bayes assumes independence between features for model building

http://statweb.stanford.edu/~tibs/ElemStatLearn/


---

## Why linear discriminant analysis?

$$log \frac{Pr(Y = k | X=x)}{Pr(Y = j | X=x)}$$
$$ = log \frac{f_k(x)}{f_j(x)} + log \frac{\pi_k}{\pi_j}$$
$$ = log \frac{\pi_k}{\pi_j} - \frac{1}{2}(\mu_k + \mu_j)^T \Sigma^{-1}(\mu_k + \mu_j)$$
$$ + x^T \Sigma^{-1} (\mu_k - \mu_j)$$

http://statweb.stanford.edu/~tibs/ElemStatLearn/


---

## Decision boundaries

<img class="center" src="../../assets/img/ldaboundary.png" height=500>

---

## Discriminant function

$$\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k \Sigma^{-1}\mu_k + log(\mu_k)$$


* Decide on class based on $\hat{Y}(x) = argmax_k \delta_k(x)$
* We usually estimate parameters with maximum likelihood


---

## Naive Bayes

Suppose we have many predictors, we would want to model: $P(Y = k | X_1,\ldots,X_m)$

We could use Bayes Theorem to get:

$$P(Y = k | X_1,\ldots,X_m) = \frac{\pi_k P(X_1,\ldots,X_m| Y=k)}{\sum_{\ell = 1}^K P(X_1,\ldots,X_m | Y=k) \pi_{\ell}}$$
$$ \propto \pi_k P(X_1,\ldots,X_m| Y=k)$$

This can be written:

$$P(X_1,\ldots,X_m, Y=k) = \pi_k P(X_1 | Y = k)P(X_2,\ldots,X_m | X_1,Y=k)$$
$$ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k) P(X_3,\ldots,X_m | X_1,X_2, Y=k)$$
$$ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k)\ldots P(X_m|X_1\ldots,X_{m-1},Y=k)$$

We could make an assumption to write this:

$$ \approx \pi_k P(X_1 | Y = k) P(X_2 | Y = k)\ldots P(X_m |,Y=k)$$

---

## Example: Iris Data

```{r iris, cache=TRUE}
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
```


---

## Create training and test sets

```{r trainingTest, dependson="iris",cache=TRUE}
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```

---

## Build predictions

```{r fit,dependson="trainingTest"}
install.packages("klaR")
library(klaR)
modlda = train(Species ~ .,data=training,method="lda")
#linear discriminant analysis
modnb = train(Species ~ ., data=training,method="nb")
#naive bayes
plda = predict(modlda,testing); pnb = predict(modnb,testing)
table(plda,pnb)
```

vary similar results
---

## Comparison of results

```{r,dependson="fit",fig.height=4,fig.width=4}
equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
```

---

## Notes and further reading

* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Model based clustering](http://www.stat.washington.edu/raftery/Research/PDF/fraley2002.pdf)
* [Linear Discriminant Analysis](http://en.wikipedia.org/wiki/Linear_discriminant_analysis)
* [Quadratic Discriminant Analysis](http://en.wikipedia.org/wiki/Quadratic_classifier)

!!!!!!!!!




# Week3 Quiz


```{r}
install.packages("pgmm")
library(AppliedPredictiveModeling)
library(caret)
library(ElemStatLearn)
library(pgmm)
library(rpart)
```



## Question 1
```{r}
data(segmentationOriginal)
```

1. Subset the data to a training set and testing set based on the Case variable in the data set.

2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings.

3. In the final model what would be the final model prediction for cases with the following variable values:

a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2

b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100

c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100

d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2

```{r}
set.seed(125)
Data = segmentationOriginal
inTrain = createDataPartition(Data$Case, p = 3/4)[[1]]
training = Data[Data$Case == "Train", ]
testing = Data[Data$Case == "Test", ]
set.seed(125)
cFit = train(Class ~., data = training, method = "rpart")
cFit$finalModel
#a = PS, WS, PS, not possible to predict
```




## Question 3

```{r}
library(pgmm)
data(olive)
olive = olive[,-1]
head(olive)
```

These data contain information on 572 different Italian olive oils from multiple regions in Italy. Fit a classification tree where Area is the outcome variable. Then predict the value of area for the following data frame using the tree command with all defaults

```{r}
newdata = as.data.frame(t(colMeans(olive)))
```

```{r}
cFit = train(Area ~ ., data = olive, method = "rpart")
ndat = predict(cFit, newdata)
ndat
```
strange result - factor variable resulting in numeric variable with prediction?


## Question 4

Load the South Africa Heart Disease Data and create training and test sets with the following code

```{r}
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
```
Then set the seed to 13234 and fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors. Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:

```{r}
library(caret)
set.seed(13234)
mFit = train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = "binomial")



missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}

predT = predict(mFit, testSA)
missClass(testSA, predT)/length(predT)
missClass(trainSA, predT)/length(predT)

confusionMatrix(table(testSA$chd,(predict(mFit,testSA) >0.5)*1))
confusionMatrix(table(trainSA$chd,(predict(mFit,trainSA) >0.5)*1))
```

## Question 5
Load the vowel.train and vowel.test data sets:

```{r}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
```


Set the variable y to be a factor variable in both the training and test set. Then set the seed to 33833. Fit a random forest predictor relating the factor variable y to the remaining variables. Read about variable importance in random forests here: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr The caret package uses by default the Gini importance.

Calculate the variable importance using the varImp function in the caret package. What is the order of variable importance?

[NOTE: Use randomForest() specifically, not caret, as there's been some issues reported with that approach. 11/6/2016]

```{r}
set.seed(33833)
rFit = train(y ~ ., data = vowel.train, method = "rf", prox = T)
rFit
varImp(rFit)
```

